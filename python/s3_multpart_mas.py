#!/usr/bin/env python3

import os
import sys
import time
import logging
import boto3
import botocore
import random
import string
from botocore.exceptions import ClientError
from concurrent.futures import ThreadPoolExecutor, as_completed

# ================== –ù–ê–°–¢–†–û–ô–ö–ò ==================
BUCKET_NAME = "31d5eb06-baket-backup-test"
FILE_PATH = "/mnt/dbaas/tmp/test-bigfile.bin"
FILE_SIZE_GB = 1          # –º–æ–∂–Ω–æ –º–µ–Ω—è—Ç—å
PART_SIZE_MB = 20        # —Ä–∞–∑–º–µ—Ä —á–∞—Å—Ç–∏ –¥–ª—è multipart
REGION = "ru-1"
ENDPOINT_URL = "https://s3.twcstorage.ru"
PARALLEL_FILES = 10         # üîπ –°–∫–æ–ª—å–∫–æ —Ñ–∞–π–ª–æ–≤ –≥—Ä—É–∑–∏–º –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ (2‚Äì5)

# ================== –õ–û–ì–ò ==================
LOG_FILE_MAIN = "s3_multipart.log"
LOG_FILE_ERRORS = "s3_errors.log"
LOG_FILE_REQUESTS = "s3_requests.log"

formatter = logging.Formatter("%(asctime)s [%(levelname)s] %(name)s: %(message)s")

# –æ–±—â–∏–π –ª–æ–≥
logger = logging.getLogger("s3tester")
logger.setLevel(logging.DEBUG)
fh_main = logging.FileHandler(LOG_FILE_MAIN, mode="a", encoding="utf-8")
fh_main.setLevel(logging.DEBUG)
fh_main.setFormatter(formatter)
ch = logging.StreamHandler(sys.stdout)
ch.setLevel(logging.INFO)
ch.setFormatter(formatter)
logger.addHandler(fh_main)
logger.addHandler(ch)

# –æ—à–∏–±–∫–∏
error_logger = logging.getLogger("s3errors")
error_logger.setLevel(logging.ERROR)
fh_err = logging.FileHandler(LOG_FILE_ERRORS, mode="a", encoding="utf-8")
fh_err.setLevel(logging.ERROR)
fh_err.setFormatter(formatter)
error_logger.addHandler(fh_err)

# –∑–∞–ø—Ä–æ—Å—ã boto3/botocore
req_logger = logging.getLogger("botocore")
req_logger.setLevel(logging.DEBUG)
fh_req = logging.FileHandler(LOG_FILE_REQUESTS, mode="a", encoding="utf-8")
fh_req.setLevel(logging.DEBUG)
fh_req.setFormatter(formatter)
req_logger.addHandler(fh_req)

# ================== S3 –∫–ª–∏–µ–Ω—Ç ==================
s3 = boto3.client(
    "s3",
    endpoint_url=ENDPOINT_URL,
    region_name=REGION,
    aws_access_key_id="B2UW7PYZZ3CDL9LKCROQ",
    aws_secret_access_key="YdFn798ci9iwWxLCWzO62gaqrPBhbbAcUaCq7w7a",
)


def generate_file(path: str, size_gb: int):
    """–°–æ–∑–¥–∞—ë—Ç —Ç–µ—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ (–µ—Å–ª–∏ –Ω–µ—Ç)."""
    if os.path.exists(path):
        logger.info(f"–§–∞–π–ª {path} —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–æ–∑–¥–∞–Ω–∏–µ")
        return

    size_bytes = size_gb * 1024 * 1024 * 1024
    logger.info(f"–°–æ–∑–¥–∞–Ω–∏–µ —Ñ–∞–π–ª–∞ {path} —Ä–∞–∑–º–µ—Ä–æ–º {size_gb} –ì–ë (~{size_bytes:,} –±–∞–π—Ç)")
    with open(path, "wb") as f:
        chunk = os.urandom(1024 * 1024)  # 1 MB —Å–ª—É—á–∞–π–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        written = 0
        while written < size_bytes:
            f.write(chunk)
            written += len(chunk)
            if written % (1024 * 1024 * 1024) == 0:
                logger.info(f"–°–æ–∑–¥–∞–Ω–æ {written // (1024*1024*1024)} –ì–ë")
    logger.info("–§–∞–π–ª —Å–æ–∑–¥–∞–Ω")


def multipart_upload(file_path: str, bucket: str, key: str, part_size_mb: int = 100):
    """–ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–∞ –≤ S3 –ø–æ multipart upload."""
    upload_id = None
    try:
        logger.info(f"[{key}] –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è multipart upload")
        mpu = s3.create_multipart_upload(Bucket=bucket, Key=key)
        upload_id = mpu["UploadId"]
        logger.info(f"[{key}] UploadId={upload_id}")

        parts = []
        part_size = part_size_mb * 1024 * 1024
        total_size = os.path.getsize(file_path)
        total_parts = (total_size + part_size - 1) // part_size
        logger.info(f"[{key}] –†–∞–∑–º–µ—Ä {total_size:,} –±–∞–π—Ç, —á–∞—Å—Ç–µ–π {total_parts}")

        with open(file_path, "rb") as f:
            part_number = 1
            while True:
                data = f.read(part_size)
                if not data:
                    break

                logger.info(f"[{key}] –ó–∞–≥—Ä—É–∑–∫–∞ —á–∞—Å—Ç–∏ {part_number}/{total_parts}, {len(data)} –±–∞–π—Ç")
                response = s3.upload_part(
                    Bucket=bucket,
                    Key=key,
                    PartNumber=part_number,
                    UploadId=upload_id,
                    Body=data
                )
                etag = response["ETag"]
                logger.info(f"[{key}] –ß–∞—Å—Ç—å {part_number} –∑–∞–≥—Ä—É–∂–µ–Ω–∞, ETag={etag}")
                parts.append({"PartNumber": part_number, "ETag": etag})
                part_number += 1

        logger.info(f"[{key}] –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ multipart upload")
        result = s3.complete_multipart_upload(
            Bucket=bucket,
            Key=key,
            UploadId=upload_id,
            MultipartUpload={"Parts": parts}
        )
        logger.info(f"[{key}] –ó–∞–≥—Ä—É–∑–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {result}")

    except botocore.exceptions.ClientError as e:
        msg = f"S3 ClientError –ø—Ä–∏ multipart upload ({key}): {e}"
        logger.error(msg)
        error_logger.exception(msg)
        if upload_id:
            logger.warning(f"–ü—Ä–µ—Ä—ã–≤–∞–Ω–∏–µ multipart upload {upload_id}")
            s3.abort_multipart_upload(Bucket=bucket, Key=key, UploadId=upload_id)
        raise
    except botocore.exceptions.EndpointConnectionError as e:
        msg = f"–û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ endpoint –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {key}: {e}"
        logger.error(msg)
        error_logger.exception(msg)
        raise
    except Exception as e:
        msg = f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ multipart upload ({key}): {e}"
        logger.exception(msg)
        error_logger.exception(msg)
        if upload_id:
            logger.warning(f"–ü—Ä–µ—Ä—ã–≤–∞–Ω–∏–µ multipart upload {upload_id}")
            s3.abort_multipart_upload(Bucket=bucket, Key=key, UploadId=upload_id)
        raise


def upload_one_file(iteration: int, file_path: str, bucket: str, part_size_mb: int, idx: int):
    """–ó–∞–≥—Ä—É–∑–∫–∞ –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ —Å —É–Ω–∏–∫–∞–ª—å–Ω—ã–º –∫–ª—é—á–æ–º."""
    try:
        object_key = f"test-bigfile-{iteration}-{idx}-" + \
                     "".join(random.choices(string.ascii_lowercase + string.digits, k=6)) + ".bin"
        logger.info(f"[TASK {idx}] –ù–∞—á–∞–ª–æ –∑–∞–≥—Ä—É–∑–∫–∏ {object_key}")

        multipart_upload(file_path, bucket, object_key, part_size_mb)

        logger.info(f"[TASK {idx}] –£–¥–∞–ª–µ–Ω–∏–µ –æ–±—ä–µ–∫—Ç–∞ {object_key} –∏–∑ S3")
        s3.delete_object(Bucket=bucket, Key=object_key)
        logger.info(f"[TASK {idx}] –û–±—ä–µ–∫—Ç {object_key} —É–¥–∞–ª—ë–Ω")

    except Exception as e:
        msg = f"–û—à–∏–±–∫–∞ –≤ –∏—Ç–µ—Ä–∞—Ü–∏–∏ {iteration}, –æ–±—ä–µ–∫—Ç {object_key}: {e}"
        logger.exception(msg)
        error_logger.exception(msg)
        raise

def main():
    generate_file(FILE_PATH, FILE_SIZE_GB)
    iteration = 1

    while True:
        try:
            logger.info(f"=== –ò—Ç–µ—Ä–∞—Ü–∏—è {iteration}: –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ {PARALLEL_FILES} —Ñ–∞–π–ª–æ–≤ ===")

            with ThreadPoolExecutor(max_workers=PARALLEL_FILES) as executor:
                futures = [
                    executor.submit(upload_one_file, iteration, FILE_PATH, BUCKET_NAME, PART_SIZE_MB, idx+1)
                    for idx in range(PARALLEL_FILES)
                ]
                for future in as_completed(futures):
                    try:
                        future.result()
                    except Exception as e:
                        logger.error(f"–û—à–∏–±–∫–∞ –≤ –∑–∞–¥–∞—á–µ: {e}")

            logger.info(f"–ò—Ç–µ—Ä–∞—Ü–∏—è {iteration} –∑–∞–≤–µ—Ä—à–µ–Ω–∞, –ø–∞—É–∑–∞ 5 —Å–µ–∫—É–Ω–¥\n")
            iteration += 1
            time.sleep(5)

        except KeyboardInterrupt:
            logger.info("–û—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø–æ Ctrl+C")
            break
        except Exception as e:
            logger.exception(f"–û—à–∏–±–∫–∞ –≤ –∏—Ç–µ—Ä–∞—Ü–∏–∏ {iteration}: {e}")
            time.sleep(10)


if __name__ == "__main__":
    main()
